{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1323f9f6-b106-4c2e-966f-4d6490bda315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UI shape: (609, 6298) nnz: 48577\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93b3acfa24f94c3793090f5ef73b12d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SWAPPED: True\n",
      "METRICS: {'precision_at_10': 0.15259999999999999, 'recall_at_10': 0.15034830920398098, 'map_at_10': 0.10818761841773746}\n",
      "üèÉ View run ALS_feedback_20250924_200925 at: http://mlflow:5001/#/experiments/1/runs/d6c157b211e54e4485650e11fa2186b6\n",
      "üß™ View experiment at: http://mlflow:5001/#/experiments/1\n",
      "Saved model: als_20250924_200925.npz and updated model_latest.npz\n",
      "API reload error: HTTPConnectionPool(host='localhost', port=8080): Max retries exceeded with url: /admin/reload-model (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7db28e8b48b0>: Failed to establish a new connection: [Errno 111] Connection refused'))\n"
     ]
    }
   ],
   "source": [
    "# === Retrain ALS using feedback.jsonl ‚Üí log to MLflow ‚Üí deploy best (model_latest.npz) ===\n",
    "import os, json, time, pathlib, requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 1) Ustawienia i ≈õcie≈ºki\n",
    "os.environ.setdefault(\"OPENBLAS_NUM_THREADS\", \"1\")\n",
    "os.environ.setdefault(\"OMP_NUM_THREADS\", \"1\")\n",
    "try:\n",
    "    from threadpoolctl import threadpool_limits\n",
    "    threadpool_limits(1, \"blas\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "ART = pathlib.Path(\"/workspace/artifacts\")\n",
    "DATA = pathlib.Path(\"/workspace/data/raw/ml-latest-small\")\n",
    "ART.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "RATINGS_CSV = DATA / \"ratings.csv\"\n",
    "USERS_MAP   = ART / \"users_map.csv\"\n",
    "ITEMS_MAP   = ART / \"items_map.csv\"\n",
    "FEEDBACK    = ART / \"feedback.jsonl\"\n",
    "POPULAR_CSV = ART / \"popular_items.csv\"\n",
    "MODEL_LATEST= ART / \"model_latest.npz\"     # to ≈Çaduje API (hot-reload)\n",
    "BACKUP_DIR  = ART / \"models\"\n",
    "BACKUP_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "API_URL = os.getenv(\"API_URL\", \"http://localhost:8080\")  # lub http://api:8080 z kontenera\n",
    "\n",
    "# 2) Dane + mapy\n",
    "ratings = pd.read_csv(RATINGS_CSV)\n",
    "assert USERS_MAP.exists() and ITEMS_MAP.exists(), \"Brak users_map.csv / items_map.csv ‚Äì uruchom najpierw notebook treningowy.\"\n",
    "users_map = pd.read_csv(USERS_MAP)\n",
    "items_map = pd.read_csv(ITEMS_MAP)\n",
    "\n",
    "# 3) Feedback (üëç/üëé) ‚Üí DataFrame\n",
    "fb_rows = []\n",
    "if FEEDBACK.exists():\n",
    "    with FEEDBACK.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                fb_rows.append(json.loads(line))\n",
    "            except Exception:\n",
    "                pass\n",
    "feedback = pd.DataFrame(fb_rows) if fb_rows else pd.DataFrame(columns=[\"user_id\",\"item_index\",\"movieId\",\"relevant\"])\n",
    "\n",
    "# 4) Zbuduj macierz UI (implicit) z wagami feedbacku\n",
    "#    - bazowo: rating>=4.0 ‚Üí waga 1.0\n",
    "#    - üëé: para user-item jest usuwana (ban)\n",
    "#    - üëç: dodajemy tƒô parƒô z wagƒÖ alpha_up (np. 2.0)\n",
    "alpha_up = 2.0\n",
    "\n",
    "# bazowe pozytywy\n",
    "pos = ratings[ratings[\"rating\"] >= 4.0][[\"userId\",\"movieId\"]].copy()\n",
    "pos[\"rating\"] = 1.0\n",
    "\n",
    "# mapuj na indeksy wg istniejƒÖcych map (KRYTYCZNE: zgodno≈õƒá z API)\n",
    "df = (pos.merge(users_map, on=\"userId\", how=\"inner\")\n",
    "         .merge(items_map[[\"item_index\",\"movieId\"]], on=\"movieId\", how=\"inner\"))\n",
    "u = df[\"user_index\"].to_numpy(np.int32)\n",
    "i = df[\"item_index\"].to_numpy(np.int32)\n",
    "v = df[\"rating\"].to_numpy(np.float32)\n",
    "\n",
    "# feedback ‚Üí map indeks√≥w\n",
    "if not feedback.empty:\n",
    "    fb = (feedback\n",
    "          .merge(users_map.rename(columns={\"userId\":\"user_id\"}), on=\"user_id\", how=\"inner\")\n",
    "          .merge(items_map[[\"item_index\",\"movieId\"]], on=\"item_index\", how=\"left\"))\n",
    "    # üëé\n",
    "    bans = set(fb[fb[\"relevant\"]==False][[\"user_index\",\"item_index\"]].dropna().astype(int)\n",
    "               .itertuples(index=False, name=None))\n",
    "    if bans:\n",
    "        keep = np.array([(int(uu),int(ii)) not in bans for uu,ii in zip(u,i)], dtype=bool)\n",
    "        u, i, v = u[keep], i[keep], v[keep]\n",
    "    # üëç\n",
    "    ups = fb[fb[\"relevant\"]==True][[\"user_index\",\"item_index\"]].dropna().astype(int).to_numpy()\n",
    "    if len(ups):\n",
    "        u = np.concatenate([u, ups[:,0]])\n",
    "        i = np.concatenate([i, ups[:,1]])\n",
    "        v = np.concatenate([v, np.full(len(ups), float(alpha_up), dtype=np.float32)])\n",
    "\n",
    "n_users = int(users_map[\"user_index\"].max()) + 1\n",
    "n_items = int(items_map[\"item_index\"].max()) + 1\n",
    "UI = csr_matrix((v, (u, i)), shape=(n_users, n_items), dtype=np.float32)\n",
    "print(\"UI shape:\", UI.shape, \"nnz:\", UI.nnz)\n",
    "\n",
    "# 5) Split + BM25\n",
    "rows, cols = UI.nonzero()\n",
    "idx_all = np.arange(UI.nnz, dtype=np.int64)\n",
    "train_idx, test_idx = train_test_split(idx_all, test_size=0.2, random_state=42)\n",
    "\n",
    "def build_sparse(indices):\n",
    "    r = rows[indices]; c = cols[indices]; vv = UI.data[indices]\n",
    "    return csr_matrix((vv, (r, c)), shape=UI.shape, dtype=np.float32)\n",
    "\n",
    "UI_train = build_sparse(train_idx)\n",
    "UI_test  = build_sparse(test_idx)\n",
    "\n",
    "from implicit.nearest_neighbours import bm25_weight\n",
    "UIw = bm25_weight(UI_train, K1=1.2, B=0.75).astype(np.float32)\n",
    "IUw = UIw.T.tocsr()  # ITEM√óUSER\n",
    "\n",
    "# 6) Trening ALS (+ swap-safe utils)\n",
    "from implicit.als import AlternatingLeastSquares\n",
    "factors, reg, iters, k_eval = 64, 0.02, 15, 10\n",
    "\n",
    "model = AlternatingLeastSquares(factors=factors, regularization=reg, iterations=iters, random_state=42)\n",
    "model.fit(IUw)\n",
    "\n",
    "n_items_train, n_users_train = UI_train.shape[1], UI_train.shape[0]\n",
    "n_items_model, _ = model.item_factors.shape\n",
    "n_users_model, _ = model.user_factors.shape\n",
    "SWAPPED = (n_items_model == n_users_train) and (n_users_model == n_items_train)\n",
    "print(\"SWAPPED:\", SWAPPED)\n",
    "\n",
    "def get_I():  # items matrix\n",
    "    return model.user_factors if SWAPPED else model.item_factors\n",
    "def get_u(uix):  # user vector\n",
    "    return model.item_factors[uix] if SWAPPED else model.user_factors[uix]\n",
    "\n",
    "# 7) Ewaluacja (P@10 / R@10 / MAP@10), maskowanie ‚Äûseen‚Äù\n",
    "truth = defaultdict(set)\n",
    "tr, tc = UI_test.nonzero()\n",
    "for r,c in zip(tr, tc): truth[r].add(c)\n",
    "\n",
    "pop_counts = np.asarray(UI_train.sum(axis=0)).ravel()\n",
    "pop_order = np.argsort(-pop_counts)\n",
    "\n",
    "def rec(uix, N=10):\n",
    "    I = get_I()\n",
    "    n = I.shape[0]\n",
    "    if UI_train.getrow(uix).nnz == 0:\n",
    "        return pop_order[:N].tolist()\n",
    "    s = I @ get_u(uix)\n",
    "    seen = [x for x in UI_train.getrow(uix).indices if x < n]\n",
    "    if seen: s[seen] = -1e12\n",
    "    top = np.argpartition(-s, min(N, n-1))[:N]\n",
    "    return top[np.argsort(-s[top])].tolist()\n",
    "\n",
    "users = np.array(list(truth.keys()))\n",
    "if len(users) > 500:\n",
    "    rng = np.random.default_rng(42)\n",
    "    users = rng.choice(users, size=500, replace=False)\n",
    "\n",
    "precs, recs, maps = [], [], []\n",
    "for uix in users:\n",
    "    t = {i for i in truth[uix] if i < get_I().shape[0]}\n",
    "    if not t: continue\n",
    "    p = rec(uix, k_eval)\n",
    "    inter = len(set(p) & t)\n",
    "    precs.append(inter/k_eval)\n",
    "    recs.append(inter/len(t))\n",
    "    hits=0; score=0.0\n",
    "    for rank,item in enumerate(p, start=1):\n",
    "        if item in t:\n",
    "            hits += 1; score += hits/rank\n",
    "    maps.append(score / min(k_eval, len(t)))\n",
    "\n",
    "metrics = {\n",
    "    \"precision_at_10\": float(np.mean(precs)) if precs else 0.0,\n",
    "    \"recall_at_10\": float(np.mean(recs)) if recs else 0.0,\n",
    "    \"map_at_10\": float(np.mean(maps)) if maps else 0.0,\n",
    "}\n",
    "print(\"METRICS:\", metrics)\n",
    "\n",
    "# 8) Zapis modelu (backup + latest), popularno≈õƒá i log do MLflow\n",
    "ts = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "backup_path = BACKUP_DIR / f\"als_{ts}.npz\"\n",
    "np.savez_compressed(backup_path, user_factors=model.user_factors, item_factors=model.item_factors)\n",
    "# podmie≈Ñ latest\n",
    "MODEL_LATEST.write_bytes(backup_path.read_bytes())\n",
    "\n",
    "# popularno≈õƒá do miksu\n",
    "pop = (ratings[ratings[\"rating\"]>=4]\n",
    "       .groupby(\"movieId\").size().rename(\"count\").reset_index())\n",
    "pop = items_map.merge(pop, on=\"movieId\", how=\"left\").fillna({\"count\":0})\n",
    "pop = pop.sort_values(\"count\", ascending=False).reset_index(drop=True)\n",
    "pop.to_csv(POPULAR_CSV, index=False)\n",
    "\n",
    "# MLflow\n",
    "import mlflow\n",
    "mlflow.set_experiment(\"netflix-poc\")\n",
    "with mlflow.start_run(run_name=f\"ALS_feedback_{ts}\"):\n",
    "    mlflow.log_param(\"factors\", factors)\n",
    "    mlflow.log_param(\"regularization\", reg)\n",
    "    mlflow.log_param(\"iterations\", iters)\n",
    "    mlflow.log_param(\"alpha_upvote\", alpha_up)\n",
    "    mlflow.log_param(\"swapped_detected\", bool(SWAPPED))\n",
    "    mlflow.log_metric(\"precision_at_10\", metrics[\"precision_at_10\"])\n",
    "    mlflow.log_metric(\"recall_at_10\", metrics[\"recall_at_10\"])\n",
    "    mlflow.log_metric(\"map_at_10\", metrics[\"map_at_10\"])\n",
    "    mlflow.log_artifact(str(backup_path))\n",
    "    mlflow.log_artifact(str(USERS_MAP))\n",
    "    mlflow.log_artifact(str(ITEMS_MAP))\n",
    "    mlflow.log_artifact(str(POPULAR_CSV))\n",
    "\n",
    "print(\"Saved model:\", backup_path.name, \"and updated\", MODEL_LATEST.name)\n",
    "\n",
    "# 9) Hot-reload modelu w API\n",
    "try:\n",
    "    r = requests.post(f\"{API_URL}/admin/reload-model\", timeout=5)\n",
    "    print(\"API reload:\", r.status_code, r.text[:200])\n",
    "except Exception as e:\n",
    "    print(\"API reload error:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3c83bc-1256-4fb2-b4c2-9e36dc5d617c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
